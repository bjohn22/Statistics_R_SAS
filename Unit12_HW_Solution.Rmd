---
title: "Unit 12 HW"
author: "Olanipekun"
date: "`r Sys.Date()`"
output: word_document
---

```{r, error=TRUE}

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Logistic Regression Conceptual questions

  1.  Explain why logistic regression is needed to model a binary, categorical response.  Use terminology like "log odds" or "the logit function".
  
| Multiple Linear Regression is more suitable for linear relationships and output a continuous value except we manually bin the into proportions.
|  Logistic Regression can handle non-linear relationships because the non-linearity is fully captured in the link function (logit (pi)) and it utilizes the  Generalized Linear Model because of that instead of the Ordinary Least Squares that is a straight linear line.
| MLR cannot predict probability that ranges from 0 to 1 but logistic regression is built for that given its sigmoid function.
| The probabilities are calculated usingthe Maximum Likelihood parameter estimation.
  
| The logit function is simply the odds of Y = 1 for binary response of 0 and 1. We simply compute the odds from the counts of the responses. And determine the effect sizes of the categorical variables much like we do in multiple linear regression.
|  Odds can easily be derived by taking the inverse of the logit(pi) function.
  
  2.  TRUE or FALSE?  Like LDA, logistic regression can only be used on continuous variables.
  
|  FALSE
  
  
  3.  When using logistic regression in software, what is the underlying prediction being made?  Is it a "Yes/No" prediction or something else?  Explain.
  
|  It predicts a probability of a Yes/No and the values range from 0 to 1 with certain threshold (e.g., 0.5) set for the prediction of the respective response.
|  It utilizes the Maximum Likelihood Parameter Estimation to predict the probability of a response.
  
  
  4.  TRUE or FALSE?  Multicollinearity is not really a problem for LDA unless there is perfect correlation between predictors.  It is however a problem for logistic regression, especially as far as interpretation is concerned.
| True. Multicollinearity still happens in logistic regression, so removing redundancies and screening noninformative variables is good practice.

  
  
  

## Exercise #1  EDA for Logistic Regression
Previously we looked at the the auto data set for multiple linear regression.  For illustration purposes, we will use this again but consider the response variable mpg as a binary categorical variable that denotes if the vehicle has "High" or "Low" mpg.  The following code recodes mpg as well as  cleans up a few variables.  I've removed the 3 cylinder and 5 cylinder vehicles as well as converted the Origin predictor to just have 2 levels (1- US or 2- Not US).  For this exercise, the "success" outcome for the response will be the "high" mpg category.  I've also gone ahead to create a training and test set for later parts of the HW.   

```{r}
library(ISLR)
newAuto<-Auto
#Creating a categorical response for illustrative purposes. High is mpg > median (mpg).
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"),levels=c("Low","High"))  #last level is the success

newAuto$cylinders<-factor(newAuto$cylinders)

#Removing data with 3 and 5 cylinders and making sure are R treats the predictor with just 3 levels and not 5.
newAuto<-newAuto[-which(newAuto$cylinders %in% c(3,5)),]
newAuto$cylinders<-factor(newAuto$cylinders)

str(newAuto)

#Creating just two Origins by combining 2 and 3 again for keeping things simple.It assigns 2 to value of 3 in Origin column
newAuto$origin[which(newAuto$origin==3)]<-2
newAuto$origin<-factor(newAuto$origin)

#From here we are going to do a simple split (250 and 135) of the data set and explore the training
#data set newAuto. And we remove the Name category with 304 levels. 
#The test will be held out for assessment of the model fits.
set.seed(1234)
index<-sample(x=1:385, size=250,replace=FALSE)
test<-newAuto[-index,-9]
train<-newAuto[index,-9]
dim(test)
```


The one key piece of code that should not go unnoticed from the above script is setting the levels for your response variable (3rd line).  Since we want the "success" outcome to be identified as High mpg, we have to make sure the last level of the factor is the success outcome.  Default is alphabetical order so if we did not force it, the success would be defined as "Low" and the interpretation of odds ratios for logistic regression models will have to reflect that difference.  You also need to keep this information consistent when looking at prediction performance graphics like ROC curves which we will discuss later.


EDA for logistic regression is typically done in a very similar fashion as multiple linear regression, the key is just making sure you view things through the lens of a categorical response.  To get a global view of a lot of predictors at once, a scatter plot matrix can be viewed.  The package GGally provides a great way to view the categorical response and takes into account the data types of each predictor (low-orange , high-blue).

```{r}
library(GGally)
#This excludes the reponse in the plots.
ggpairs(newAuto,columns=2:8,aes(colour=mpg))

```

| We can also conduct PCA using the continuous predictors as previously discussed.  
| Summary statistics should also be viewed as well as clustering tools which we will get in to a little later. 
| If you want to take a closer look at some of the plots in the scatter matrix, the following codes can create individual plots and statistics to explore trends.  
| In this examples, we can see that the percentage of high mpg vehicles increases as the number of cylinders decreases. 
| This is also obvious in plot r= 1: c= 1 of the pairs() output.
| A similar trend can be observed with boxplots of weight.


```{r}
attach(newAuto)

#Works only for categorical. Works for Cont. if we aggregate.
prop.table(table(mpg,cylinders),2)
plot(mpg~cylinders,col=c("red","blue"))

#Using the summary stats to view the proportion of the responses.
#And it knows to use boxplot without needing to specify.
t(aggregate(weight~mpg,data=newAuto,summary))
plot(weight~mpg,col=c("red","blue"))
```

5.  Using the summary statistics and scatter matrix provided previously or creating some of your own, briefly describe if any of the predictors look like they could be helpful in predicting the mpg categories (outside of cylinders and weight which was already mentioned).

| My responses
Using the prop tables and plots are quite intuitive and I will use them here.


```{r}
attach(newAuto)

#Which ones do I need to factor before using the prop table
sapply(newAuto, function(x) length(unique(x)))
par(mfrow=c(2,2))
#Displacement (continuous), Horsepower (continuous), acceleration (continuous/discrete) and Year(discrete, higher= greater)
str(newAuto)

#Year
prop.table(table(mpg,year),2)
plot(mpg~year,col=c("red","blue"), main ="mpg versus year")


#Displacement
t(aggregate(displacement~mpg,data=newAuto,summary))
plot(displacement~mpg,col=c("red","blue"), main = "mpg versus displacement")



#Horsepower
t(aggregate(horsepower~mpg,data=newAuto,summary))
plot(horsepower~mpg,col=c("red","blue"), main = "mpg versus horsepower")


#Acceleration
t(aggregate(acceleration~mpg,data=newAuto,summary))
plot(acceleration~mpg,col=c("red","blue"), main = "mpg versus acceleration")




```

For the attributes analyzed:

Year: The proportion of newer cars with high mpg is much more than that of older cars

Displacement:
Displacement of <100 favors high mpg cars. Anything higher favors low mpg cars

Horsepower:
Cars with relatively lower (<75) tend to have high mpg compared to their counterparts with higher horsepower (> about 100 horsepower)  

Acceleration:
Broadly speaking, cars with high mpg tend to record higher the acceleration. 





6.  We discussed previously that multicollinearity can happen among categorical as well as continuous predictors.  Our focus has been on exploration of the continuous variables mainly.  For example, create a boxplot of weight by cylinders.  If there is no multicollinearity then the average weight should not depend on the cylinder of the car.  What does your graph suggest?  What about origin and cylinders... are they "correlated" visually? 



```{r}
attach(newAuto)

#Checking for multicollinearity


par(mfrow=c(2,1))
#Weight (continuous), cylinders (categorical), origin (categorical)

#cylinder,origin
prop.table(table(origin,cylinders),2)
plot(origin~cylinders,col=c("black","cyan"), main ="cylinders versus origin (US =1, Non US =2)")



#weight, cylinder
t(aggregate(weight~cylinders,data=newAuto,summary))
plot(weight~cylinders, col=c("black","cyan", "green"), main = "cylinder versus weight")

```


## Multicollinearity

#### Cylinders and Origin  
| Using the proportion table and its plot  
| The proportion of cars from a particular origin varies with the number of cylinders.   
| For example, almost all the 8-cylinder vehicles come from US. while 65% of 4-cylinder cars come from Non-US origin.    
| This indicates multicolinearity between these two attributes.  

#### Cylinders and Weight  
| From the boxplot  
| There is multicollinearity because the higher the cylinder, the greater the weight of the vehicles   




## Exercise #2  Simple logistic regression (making connections).
Logistic regression allows for a more general strategy for modeling numerous predictors when the response is a binary categorical variable.  When you only deal with single predictor, you will get back to previous methods that we have already covered.  For example consider the following table which examines the relationship between mpg and origin.  The 2x2 table is provided below along with the ODDS ratio estimate which is interpreted as "The odds of car having high mpg for non US vehicles is xxx times higher than that for US vehicles".  

```{r}
library(epitools)
mymat<-table(origin,mpg)
mymat
A = oddsratio.wald(mymat)
A

B = oddsratio.wald(mymat, rev = "columns")
B

C = oddsratio.wald(mymat, rev = "rows")
C
```

### Interpretation  

* In A: The odds of a car having low mpg for US Vehicles is 15.3 times as large as the odds of a car having low mpg for non-US vehicles. 
* In B: The odds of a car having high mpg for US Vehicles is 0.065 times as large as the odds of a car having high mpg for non-US vehicles.
* Lastly, the odds of a car having high mpg originating from non-US Vehicles location is 15.302 times((121/19)/(72/173) = 15.302) as large as the odds of a high MPG car originating from a US location.


Running a logistic regression call in R is very similar to ordinary multiple linear regression (MLR).  The script below provides the function call using just Origin as a predictor. The traditional "summary" function can produce the regression coefficients, z-statistics, and p-values just like in MLR.  

```{r}
simple.log<-glm(mpg~origin,family="binomial",data=newAuto)
summary(simple.log)

OR_origin.2 = exp(2.7280)
OR_origin.2

exp(cbind("Odds ratio" = coef(simple.log), confint.default(simple.log, level = 0.95)))
```



7.  Interpret the regression coefficient (in terms of an odds ratio) from the logistic regression model and compare your result to the 2x2 table analysis already covered.  What did you find?!?!  

### Interpretation.  
* In the logistic regression output: Origin=1 (US) is the reference level.  
* In comparison to the 2 x 2 table the Odds ratio of 15.302 times is interpreted in favor of High MPG in non-US vehicles. i.e:  
| The odds of a car having High MPG mpg for non-US Vehicles is 15.3 times as large as the odds of a car having high mpg for US vehicles.  
|  It is basically the same as the interpretation I provided in the 2 x 2 table.  
* Note that "success" is high mpg as forced earlier.    





## Exercise 3 Feature Selection for Logistic Regression

The same strategies from MLR can be applied to the logistic regression setting.  When working with larger sets of predictors, this will be a very common strategy to implement.  The following code performs stepwise logistic regression using AIC as a stopping criterion. Forward and backward work similarly.  While it is possible to compute a test error metric at each step or via CV, we will work with AIC on the training data set for now.  (Many packages have this pretty well automated.  See caret for example. )

```{r}
library(MASS)
library(tidyverse)
library(car)
full.log<-glm(mpg~.,family="binomial",data=train)
step.log<-full.log %>% stepAIC(trace=FALSE)
```

We can use the "coef" function to see which predictors were included.  If the stepwise model was our final model, we can also use the "summary" function for testing.  For interpretation, I've included some additional code that produces the odds ratio tables for each of the coefficients.

```{r}
summary(step.log)
exp(cbind("Odds ratio" = coef(step.log), confint.default(step.log, level = 0.95)))
vif(step.log)

#Note:  You can also look at vifs.  vif(step.log)  #Last column is interpretted based off of rule of 5 or 10
```

8.  Using the stepwise regression model, interpret the regression coefficient for cylinder 6 (note the reference category is cylinder 4) and year.  Note:  The interpretation of cylinder 8 doesn't make a whole lot of sense.  This is due to the fact that cylinders are confounded (correlated) with quite a few of the other predictor variables in the model.  Explore EDA or we can talk more in office hours on seeing this if you are having trouble.

### Interpretation.  
* The odds ratio of a high MPG car with 6 cylinders is 15.3 after accounting for weight, year and origin (95% confidence interval is 0.0486, 1.2). This is given that 8 cylinders in the model is not a statistically significant factor.

* The odds ratio that a 1 year newer car has high MPG car is 0.416 times, after accounting for cylinder, year and origin. 


LASSO is obtained in the exact same way for logistic as is MLR.  The only difference is to let R know that our response is categorical through the "family="binomial"" option.  Cross validation is used to obtain the optimal penalty value.  A final refit using the entire data set can then be obtained once the optimal penalty value is determined.  For this example, the object "finalmodel" produces the final lasso model.
```{r}
library(glmnet)
dat.train.x <- model.matrix(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin-1,train)
dat.train.y<-train[,1]
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
print("CV Error Rate:")
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
print("Penalty Value:")
cvfit$lambda.min

#For final model predictions go ahead and refit lasso using entire
#data set
finalmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)
finalmodel

```


## Exercise 4  Confusion Matrix, Prediction Metrics, and choosing a cutoff
Lets compare the stepwise and lasso models using the test set.  We have previously discussed the confusion matrix when covering LDA.  Previously in R, we worked with the LDA providing the predictions in terms of the categories labels.  However the true predictions from the models are predictive probabilities.  To help get a handle on this, the following code makes predictions on the test set using the LASSO model.  Using the first 15 observations in the test set, I've printed off the true mpg status of the observation along with their predicted probabilities from the LASSO model.  Since the probabilities are the chances a vehicle has High mpg given the set of predictor values for that observation, we can see that there is pretty good correspondence with the probabilities and the the actual true status (small probabilities for Low mpg, large probabilities for High mpg).

```{r}
dat.test.x<-model.matrix(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin-1,test)
fit.pred.lasso <- predict(finalmodel, newx = dat.test.x, type = "response")

test$mpg[1:15]
fit.pred.lasso[1:15]

#Making predictions for stepwise as well for later
fit.pred.step<-predict(step.log,newdata=test,type="response")

```


To compute the confusion matrix on the test set for the two models, you simply take the predicted probabilites and convert them to the categorical names based on some cut off value for the probabilities.  Most default settings is set to 0.5.  Once you convert the predicted probabilities to their class labels, the confusion matrix can be made.  (Note:  You could skip this step by changing the "type" option within the predict function but you should really understand whats going on here for later discussions.)

```{r}
#Lets use the predicted probablities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff<-0.5
class.lasso<-factor(ifelse(fit.pred.lasso>cutoff,"High","Low"),levels=c("Low","High"))
class.step<-factor(ifelse(fit.pred.step>cutoff,"High","Low"),levels=c("Low","High"))

#Confusion Matrix for Lasso
conf.lasso<-table(class.lasso,test$mpg)
print("Confusion matrix for LASSO")
conf.lasso

conf.step<-table(class.step,test$mpg)
print("Confusion matrix for Stepwise")
conf.step
```

We can compute the overall accuracy for the confusion matrix using the following code.  I've also included an additional code that allows for you to calculate overall accuracy without generating the table.

```{r}
#Accuracy of LASSO and Stepwise
print("Overall accuracy for LASSO and Stepwise respectively")
sum(diag(conf.lasso))/sum(conf.lasso)
x3= round(sum(diag(conf.step))/sum(conf.step) * 100,1)


print("Alternative calculations of accuracy")
#Rather than making the calculations from the table, we can compute them more quickly using the following code which just checks if the prediction matches the truth and then computes the proportion.
mean(class.lasso==test$mpg)
mean(class.step==test$mpg)

```

9.  The reason why predicted probabilities and converting them to categories to make predictions is so important is because choosing the cutoff of 0.5 is somewhat arbitrary.  In fact, if your training data set is highly imbalanced (like your project), the standard cutoff can lead to some problematic confusion matrices.  I've already suggested that you guys balance training data sets in advance.  This helps with keeping the cutoff at 0.5.  The alternative strategy is to change the cutoff.  The resulting confusion matrix from changing the cutoff could help/or hurt overall accuracy.  You just have to explore.  To get used to the idea, recompute the confusion matrix tables for the stepwise logistic regression model using a cutoff value of 0.1 and 0.9. Compare these two tables to the original tables that used a cutoff value of 0.5.  What is going on with the predictions, in particular how the misclassifications are happening? 




### My response starts here


```{r}
#Lets use the predicted probabilities to classify the observations and make a final confusion matrix for the two models.  We can use it to calculate error metrics.
#Lets use a cutoff of 0.5 to make the classification.
cutoff1<-0.1
cutoff2 <- 0.9
#class.lasso<-factor(ifelse(fit.pred.lasso>cutoff,"High","Low"),levels=c("Low","High"))
class.step1<-factor(ifelse(fit.pred.step>cutoff1,"High","Low"),levels=c("Low","High"))
class.step2<-factor(ifelse(fit.pred.step>cutoff2,"High","Low"),levels=c("Low","High"))

#Confusion Matrix for Lasso
#conf.lasso<-table(class.lasso,test$mpg)
#print("Confusion matrix for LASSO")
#conf.lasso

conf.step1<-table(class.step1,test$mpg)
print(paste("Confusion matrix for Stepwise for cut-off", cutoff1))
conf.step1

conf.step2<-table(class.step2,test$mpg)
print(paste("Confusion matrix for Stepwise for cut-off", cutoff2))
conf.step2
```

Compute the overall accuracy for the confusion matrix.
Compare these two tables to the original tables that used a cutoff value of 0.5.  What is going on with the predictions, in particular how the misclassifications are happening? 

```{r}
#Accuracy of LASSO and Stepwise
print(paste("Overall accuracy for cut off", cutoff1, "and", cutoff2, "respectively"))
#sum(diag(conf.lasso))/sum(conf.lasso)
x1 = round(sum(diag(conf.step1))/sum(conf.step1) * 100, 1)
x2 = round(sum(diag(conf.step2))/sum(conf.step2) * 100, 1)

accuracy = c(x1, x2, x3)
cut.off = c("cut off 0.1", "cut-off 0.9", "cut-off 0.5")

accuracies = cbind(cut.off, accuracy)

accuracies


```

### Comments.  

•	Our “success” class is High MPG and it much fewer than Low MPG in the test set.  
-	Class imbalance issue.     
-	This suggests that even a random guess will yield a very high accuracy (up to 90%), if it predicts low MPG. 
•	The formula for accuracy is (True Positive + True Negative)/No of instances. 
•	Lowering the threshold from 0.5 to 0.1 lowered the rate of predicting the low mpg class: 52/63 = 0.825 (cut off 0.5 is 59/63 = 0.94).  
If we are interested in predicting low mpg class, then threshold of 0.5 will be better than that of 0.1.  
•	More to this, the rate of predicting “High mpg” is higher for threshold of 0.1 (71/72 = 0.986) versus 69/72 =0.958 for threshold of 0.5.  
•	The cut off of 0.9 yields the lowest accuracy (113/135 = 0.83) mainly because of much lower accuracy in predicting the High MPG class (51/72 = 0.71). 
-	Despite the poorer accuracy, the confusion matrix shows that it is the best of the three cut offs, with almost perfect accuracy, at predicting the low MPG class (62/63 = 0.98).   
•	You gain some you lose some. So, in all, it boils down to the question of interest.    




## Sensitivity, Specificity, and the ROC Curve.  (NO more HW.  I will discuss these in class, R scripts are here so you already have some examples.)

Depending on how deep you look into #9, one observation is that overall accuracy between the cut off of 0.1 and 0.5 is not that drastic.  They are both above 90 percent, but changing the cutoff away from 0.5 in this example creates some problems.  The issue is that the sensitivity and specificty of the model becomes unbalanced.  Lets take a look at the test set confusion matrix using the cut off of 0.1 for the stepwise model.  

```{r, echo=FALSE}
cutoff<-0.1
class.step<-factor(ifelse(fit.pred.step>cutoff,"High","Low"),levels=c("Low","High"))
conf.step_0.1<-table(class.step,test$mpg)
conf.step_0.1


```


The overall accuracy is still quite good, right at 91% (123/135).  While that may sound good, there are some issues.  Looking at the table we can see that this particular cutoff yields predictions that predict the "High" category almost perfectly (71/72  98.6%).  Since "high" is our "success" label, this accuracy measurement is referred to as sensitivity.  It is also called the True Positive Rate (TPR).  Positive/Negative taking the place of the success/failure wording for the response variable.  While sensitivity here is awesome, the accuracy in predicting the "low" group is not as good (52/63  82%).  This accuracy measurement is referred to as the True Negative Rate (TRN).

If we had left the cutoff at 0.5 for this model, the sensitivity and specificity would both be around 94% and 95%.  So you get good accuracy from both groups.  When data sets are unbalanced with the response classes, even though the overall accuracy may be good using a cutoff of 0.5, the sensitivity and specificity can be wildly different. In some cases, that might be okay to accept, in other cases you may really want to consider changing the cutoff to get those sensitivity and specificity metrics to a place that is more practical for the problem at hand. I'll provide some examples in class on where you might want to focus on sensitivity or specificity as the overall metric to compare models rather than an overall accuracy measurement.

## ROC Curves
With the ability to change the cutoffs, the natural question is what would be the best cutoff to use.  There are numerous ways to do this, but one of the most common approaches is to view predictability of your models through the ROC curve.  What the ROC curve does is consider a wide range of possible cutoff values all the way from 0 up to 1.  It computes sensitivity and specificity measurements from each of the cut off values it considers.  It then plots the sensitivity measurements on the y axis versus 1-specificity on the x axis.  A plot of the ROC curve for the LASSO model is below.

```{r}
library(ROCR)
results.lasso<-prediction(fit.pred.lasso, test$mpg,label.ordering=c("Low","High"))
roc.lasso = performance(results.lasso, measure = "tpr", x.measure = "fpr")
plot(roc.lasso,colorize = TRUE)
abline(a=0, b= 1)
```

What we are generally looking for with the ROC curve is that a good model will produce good sensitivity values (TPR) close to 1 as well as good specificity values (TNR) that should should be close to 1.  Since the x-axis is 1-specficity, we would like our models to be very small on this scale.  1-specificity is also refered to as the False Positive Rate which we see in the graph.  So when we examine the curve, if a model is able to really predict well, then most of the cutoffs considered will yield points in the top left corner of the graph.  Of course if the cut off is really small or large, the sensitivity or specificity will faulter and you will get points in the bottom left and top right.  

If your model does not do very well for any cut off, then the curve will tend to hover around the 45 degree line.  This serves as the sanity check. If your model's ROC curve is there, then you might as well just randomly pick because your predictors are not really providing anything helpful. 

The color legend on the ROC curve can be turned off.  When turned on it gives you a sense of where the optimal cutoff value is.  The best balance of sensitivity and specificity values corresponds to the yellow to light green section of the curve.  This suggests that an optimal cutoff value could be somewhere around 0.6 or maybe even as high as 0.7.  When making predictions for future data sets you could use this cutoff to make predictions and validate if the cutoff is working well.

The other advantage that the ROC curve has is that it can provide a general way of comparing models.  The following code takes the LASSO, stepwise, and a simple logistic model using just 2 predictors, and overlays their ROC curves from the test set.  The LASSO and Stepwise curves are very similar.  However we can see that the simple model with only two predictors doesn't perform as well.  We could also verify this by comparing confusion matrices, overall accuracy, sensitivity, and specificity between the models for a given cutoff.
```{r}
results.step<-prediction(fit.pred.step, test$mpg,label.ordering=c("Low","High"))
roc.step = performance(results.step, measure = "tpr", x.measure = "fpr")


simple.log<-glm(mpg~origin+horsepower,family="binomial",data=train)
fit.pred.origin<-predict(simple.log,newdata=test,type="response")
results.origin<-prediction(fit.pred.origin,test$mpg,label.ordering=c("Low","High"))
roc.origin=performance(results.origin,measure = "tpr", x.measure = "fpr")

plot( roc.lasso)
plot(roc.step,col="orange", add = TRUE)
plot(roc.origin,col="blue",add=TRUE)
legend("bottomright",legend=c("Lasso","Stepwise","Origin/Horsepower Only"),col=c("black","orange","blue"),lty=1,lwd=1)
abline(a=0, b= 1)

```


Note: When fitting multiple models like in your project, it is typically accepted to choose an optimal threshold for each model to give them each the best chance to succeed.  You do not have to force yourself to use the same cutoff across the board.


